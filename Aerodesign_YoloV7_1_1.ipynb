{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Aerodesign_YoloV7_1.1.ipynb",
      "provenance": [],
      "collapsed_sections": [
        "icpTkGW7csZA"
      ]
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "![picture](https://imgur.com/CtSxsdC.png)\n",
        "\n",
        "# **UBC Aero Design Custom YoloV7 Trainer**\n",
        "\n",
        "**Dependencies**\n",
        "\n",
        "\n",
        "* Keyboard\n",
        "* Mouse\n",
        "* Brain\n",
        "* [RoboFlow Acount](https://roboflow.com) (for basic image processing and data segmentation)\n",
        "* [Labelimg](https://github.com/heartexlabs/labelImg)\n",
        "\n",
        "\n",
        "**Rough training layout**\n",
        "1. Install YOLOv7 \n",
        "2. Load custom dataset from Roboflow in YOLOv7 format\n",
        "3. Run YOLOv7 training\n",
        "4. Evaluate YOLOv7 performance\n",
        "5. Run YOLOv7 inference on non-live Video\n",
        "6. Single Image processing\n"
      ],
      "metadata": {
        "id": "9GBRlvgwh83H"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Step 1: Installing dependencies**\n",
        "\n"
      ],
      "metadata": {
        "id": "Nf68_vJKZQsy"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/gdrive')"
      ],
      "metadata": {
        "id": "yss84IkxZeQV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "%cd /content/gdrive/MyDrive\n",
        "!git clone https://github.com/augmentedstartups/yolov7.git\n",
        "%cd yolov7\n",
        "!pip install -r requirements.txt\n",
        "!pip install roboflow"
      ],
      "metadata": {
        "id": "LZSGWhLNZ0LW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "*Note*: running the second will prompt a colab restart. Restart colab and only run the first cell."
      ],
      "metadata": {
        "id": "kOLwE87tZ3yz"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Step 2: Downloading data from RoboFlow**"
      ],
      "metadata": {
        "id": "HD62OfMeaQsE"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%cd /content/gdrive/MyDrive/yolov7\n",
        "\n",
        "from roboflow import Roboflow\n",
        "rf = Roboflow(api_key=\"###Inset api key here###\")\n",
        "project = rf.workspace(\"###inset### (ex: ubc-ad)\").project(\"###inset### (ex: dots-yupww)\")\n",
        "dataset = project.version(1).download(\"yolov7\")"
      ],
      "metadata": {
        "id": "QNsYPmO4aZlS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Step 3: Let's run!**"
      ],
      "metadata": {
        "id": "PcjpiiTPaxUx"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%cd /content/gdrive/MyDrive/yolov7"
      ],
      "metadata": {
        "id": "VmP60vU6a1KF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "%%bash\n",
        "wget -P /content/gdrive/MyDrive/yolov7 https://github.com/WongKinYiu/yolov7/releases/download/v0.1/yolov7.pt\n",
        "#wget -P /content/gdrive/MyDrive/yolov7 https://github.com/WongKinYiu/yolov7/releases/download/v0.1/yolov7x.pt\n",
        "# wget -P /content/gdrive/MyDrive/yolov7 https://github.com/WongKinYiu/yolov7/releases/download/v0.1/yolov7-w6.pt\n",
        "# wget -P /content/gdrive/MyDrive/yolov7 https://github.com/WongKinYiu/yolov7/releases/download/v0.1/yolov7-e6.pt\n",
        "# wget -P /content/gdrive/MyDrive/yolov7 https://github.com/WongKinYiu/yolov7/releases/download/v0.1/yolov7-d6.pt\n",
        "# wget -P /content/gdrive/MyDrive/yolov7 https://github.com/WongKinYiu/yolov7/releases/download/v0.1/yolov7-e6e.pt"
      ],
      "metadata": {
        "id": "Bz-3jXb6a4y_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Training procedures**\n",
        "\n",
        "There are many [arguments](https://github.com/WongKinYiu/yolov7/blob/main/train.py). For our case right now, the most important ones are\n",
        "\n",
        "1. Epoch\n",
        "2. Weights\n",
        "3. Batch\n",
        "\n",
        "**Make sure that the training folder is correctly addressed!!!**"
      ],
      "metadata": {
        "id": "AYyOFUA1bAza"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%cd /content/gdrive/MyDrive/yolov7 ## Training folder location\n",
        "!python train.py --batch 15 --cfg cfg/training/yolov7.yaml --epochs 55  this --data {dataset.location}/data.yaml --weights 'yolov7.pt' --device 0 "
      ],
      "metadata": {
        "id": "JqORuZcDbxxj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Step 4: Wait, did we do that right?**\n",
        "\n",
        "Let's see"
      ],
      "metadata": {
        "id": "h7wpigVbcIXf"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from IPython.display import Image\n",
        "display(Image(\"/content/gdrive/MyDrive/yolov7/runs/train/exp/F1_curve.png\", width=400, height=400))\n",
        "display(Image(\"/content/gdrive/MyDrive/yolov7/runs/train/exp/PR_curve.png\", width=400, height=400))\n",
        "display(Image(\"/content/gdrive/MyDrive/yolov7/runs/train/exp/confusion_matrix.png\", width=500, height=500))"
      ],
      "metadata": {
        "id": "j9TfPwV7cSSd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Let's try the test images now"
      ],
      "metadata": {
        "id": "aXoKxqSNccv0"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "import glob\n",
        "from IPython.display import Image, display\n",
        "\n",
        "i = 0\n",
        "limit = 10000 # max images to print\n",
        "for imageName in glob.glob('/content/gdrive/MyDrive/yolov7/runs/detect/exp2/*.jpg'): ###change to .png if png. H264 needs more stuff to be added\n",
        "    if i < limit:\n",
        "      display(Image(filename=imageName))\n",
        "      print(\"\\n\")\n",
        "    i = i + 1\n",
        "\n",
        "display(Image(\"/content/gdrive/MyDrive/yolov7/runs/detect/exp3/52_jpg.rf.c3931652d0d6e62034543e92ec110c0b.jpg\", width=400, height=400))\n",
        "    "
      ],
      "metadata": {
        "id": "v3GhEF8bce-F"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Step 5: Custom test/Implementation**"
      ],
      "metadata": {
        "id": "icpTkGW7csZA"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**The cell below must be runned first**"
      ],
      "metadata": {
        "id": "3ZqEAlD_c4ih"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import sys\n",
        "sys.path.append('/content/gdrive/MyDrive/yolov7')\n",
        "\n",
        "\n",
        "import argparse\n",
        "import time\n",
        "from pathlib import Path\n",
        "import cv2\n",
        "import torch\n",
        "import numpy as np\n",
        "import torch.backends.cudnn as cudnn\n",
        "from numpy import random\n",
        "\n",
        "from models.experimental import attempt_load\n",
        "from utils.datasets import LoadStreams, LoadImages\n",
        "from utils.general import check_img_size, check_requirements, check_imshow, non_max_suppression, apply_classifier, \\\n",
        "    scale_coords, xyxy2xywh, strip_optimizer, set_logging, increment_path\n",
        "from utils.plots import plot_one_box\n",
        "from utils.torch_utils import select_device, load_classifier, time_synchronized, TracedModel\n",
        "\n",
        "def letterbox(img, new_shape=(640, 640), color=(114, 114, 114), auto=True, scaleFill=False, scaleup=True, stride=32):\n",
        "    shape = img.shape[:2]  # current shape [height, width]\n",
        "    if isinstance(new_shape, int):\n",
        "        new_shape = (new_shape, new_shape)\n",
        "\n",
        "    # Scale ratio (new / old)\n",
        "    r = min(new_shape[0] / shape[0], new_shape[1] / shape[1])\n",
        "    if not scaleup:  # only scale down, do not scale up\n",
        "        r = min(r, 1.0)\n",
        "\n",
        "    # Compute padding\n",
        "    ratio = r, r  # width, height ratios\n",
        "    new_unpad = int(round(shape[1] * r)), int(round(shape[0] * r))\n",
        "    dw, dh = new_shape[1] - new_unpad[0], new_shape[0] - new_unpad[1]  # wh padding\n",
        "    if auto:  # minimum rectangle\n",
        "        dw, dh = np.mod(dw, stride), np.mod(dh, stride)  # wh padding\n",
        "    elif scaleFill:  # stretch\n",
        "        dw, dh = 0.0, 0.0\n",
        "        new_unpad = (new_shape[1], new_shape[0])\n",
        "        ratio = new_shape[1] / shape[1], new_shape[0] / shape[0]  # width, height ratios\n",
        "\n",
        "    dw /= 2  # divide padding into 2 sides\n",
        "    dh /= 2\n",
        "\n",
        "    if shape[::-1] != new_unpad:  # resize\n",
        "        img = cv2.resize(img, new_unpad, interpolation=cv2.INTER_LINEAR)\n",
        "    top, bottom = int(round(dh - 0.1)), int(round(dh + 0.1))\n",
        "    left, right = int(round(dw - 0.1)), int(round(dw + 0.1))\n",
        "    img = cv2.copyMakeBorder(img, top, bottom, left, right, cv2.BORDER_CONSTANT, value=color)  # add border\n",
        "    return img, ratio, (dw, dh)"
      ],
      "metadata": {
        "id": "4Qurr8lYdB73"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Now to configure parameters. We only have one parameter that is the landing zone but this makes the model more flexible if we want to detect squrriels on the ground later on. The cell below is now essential to run right now. Just skip it. "
      ],
      "metadata": {
        "id": "JDEG4Fv5detv"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "classes_to_filter = None  #Example ['bob','stewart'] Note that capital matters. Match class to labelled image name exactly.\n",
        "\n",
        "\n",
        "opt  = {\n",
        "    \n",
        "    \"weights\": \"/content/gdrive/MyDrive/yolov7/runs/train/exp/weights/epoch_024.pt\", # Path to weights file default weights are for nano model\n",
        "    \"yaml\"   :\"###inset yaml file path###\",\n",
        "    \"img-size\": 640, # default image size\n",
        "    \"conf-thres\": 0.25, # confidence threshold for inference.\n",
        "    \"iou-thres\" : 0.45, # NMS IoU threshold for inference.\n",
        "    \"device\" : '0',  # device to run our model i.e. 0 or 0,1,2,3 or cpu\n",
        "    \"classes\" : classes_to_filter  # list of classes to filter or None\n",
        "\n",
        "}"
      ],
      "metadata": {
        "id": "YHE7LxZJdnvW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Non-live video processing**"
      ],
      "metadata": {
        "id": "JyWjR1bgeY1V"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Upload the video to a drive folder and locate the link and inset below"
      ],
      "metadata": {
        "id": "LE5-HZxme4HW"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#change URL\n",
        "%cd /content/gdrive/MyDrive/yolov7\n",
        "!gdown https://drive.google.com/uc?id=1lYhZEGGgdUjyJTIcF7W1GFMxoLfBZnfq ###Change this to a downloadable link###"
      ],
      "metadata": {
        "id": "VCrdbERyewFA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Give the full link path below"
      ],
      "metadata": {
        "id": "0gFuCiPUfFMX"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#give the full path to video, your video will be in the Yolov7 folder\n",
        "video_path = '/content/gdrive/MyDrive/yolov7/Trimmed.mp4' ###Change this to the location you see in the above cell###"
      ],
      "metadata": {
        "id": "gwjHHb2VfI76"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "##In case you didn't listen and did not run the mandatory cell\n",
        "import os\n",
        "import sys\n",
        "sys.path.append('/content/gdrive/MyDrive/yolov7') ###Make sure this is correct###\n",
        "\n",
        "\n",
        "import argparse\n",
        "import time\n",
        "from pathlib import Path\n",
        "import cv2\n",
        "import torch\n",
        "import numpy as np\n",
        "import torch.backends.cudnn as cudnn\n",
        "from numpy import random\n",
        "\n",
        "from models.experimental import attempt_load\n",
        "from utils.datasets import LoadStreams, LoadImages\n",
        "from utils.general import check_img_size, check_requirements, check_imshow, non_max_suppression, apply_classifier, \\\n",
        "    scale_coords, xyxy2xywh, strip_optimizer, set_logging, increment_path\n",
        "from utils.plots import plot_one_box\n",
        "from utils.torch_utils import select_device, load_classifier, time_synchronized, TracedModel\n",
        "\n",
        "video = cv2.VideoCapture(video_path)\n",
        "\n",
        "\n",
        "#Video information\n",
        "fps = video.get(cv2.CAP_PROP_FPS)\n",
        "w = int(video.get(cv2.CAP_PROP_FRAME_WIDTH))\n",
        "h = int(video.get(cv2.CAP_PROP_FRAME_HEIGHT))\n",
        "nframes = int(video.get(cv2.CAP_PROP_FRAME_COUNT))\n",
        "\n",
        "# Initialzing object for writing video output\n",
        "output = cv2.VideoWriter('output.mp4', cv2.VideoWriter_fourcc(*'DIVX'),fps , (w,h))\n",
        "torch.cuda.empty_cache()\n",
        "# Initializing model and setting it for inference\n",
        "with torch.no_grad():\n",
        "  weights, imgsz = opt['weights'], opt['img-size']\n",
        "  set_logging()\n",
        "  device = select_device(opt['device'])\n",
        "  half = device.type != 'cpu'\n",
        "  model = attempt_load(weights, map_location=device)  # load FP32 model\n",
        "  stride = int(model.stride.max())  # model stride\n",
        "  imgsz = check_img_size(imgsz, s=stride)  # check img_size\n",
        "  if half:\n",
        "    model.half()\n",
        "\n",
        "  names = model.module.names if hasattr(model, 'module') else model.names\n",
        "  colors = [[random.randint(0, 255) for _ in range(3)] for _ in names]\n",
        "  if device.type != 'cpu':\n",
        "    model(torch.zeros(1, 3, imgsz, imgsz).to(device).type_as(next(model.parameters())))\n",
        "\n",
        "  classes = None\n",
        "  if opt['classes']:\n",
        "    classes = []\n",
        "    for class_name in opt['classes']:\n",
        "      classes.append(opt['classes'].index(class_name))\n",
        "\n",
        "  for j in range(nframes):\n",
        "\n",
        "      ret, img0 = video.read()\n",
        "      if ret:\n",
        "        img = letterbox(img0, imgsz, stride=stride)[0]\n",
        "        img = img[:, :, ::-1].transpose(2, 0, 1)  # BGR to RGB, to 3x416x416\n",
        "        img = np.ascontiguousarray(img)\n",
        "        img = torch.from_numpy(img).to(device)\n",
        "        img = img.half() if half else img.float()  # uint8 to fp16/32\n",
        "        img /= 255.0  # 0 - 255 to 0.0 - 1.0\n",
        "        if img.ndimension() == 3:\n",
        "          img = img.unsqueeze(0)\n",
        "\n",
        "        # Inference\n",
        "        t1 = time_synchronized()\n",
        "        pred = model(img, augment= False)[0]\n",
        "\n",
        "        \n",
        "        pred = non_max_suppression(pred, opt['conf-thres'], opt['iou-thres'], classes= classes, agnostic= False)\n",
        "        t2 = time_synchronized()\n",
        "        for i, det in enumerate(pred):\n",
        "          s = ''\n",
        "          s += '%gx%g ' % img.shape[2:]  # print string\n",
        "          gn = torch.tensor(img0.shape)[[1, 0, 1, 0]]\n",
        "          if len(det):\n",
        "            det[:, :4] = scale_coords(img.shape[2:], det[:, :4], img0.shape).round()\n",
        "\n",
        "            for c in det[:, -1].unique():\n",
        "              n = (det[:, -1] == c).sum()  # detections per class\n",
        "              s += f\"{n} {names[int(c)]}{'s' * (n > 1)}, \"  # add to string\n",
        "    \n",
        "            for *xyxy, conf, cls in reversed(det):\n",
        "\n",
        "              label = f'{names[int(cls)]} {conf:.2f}'\n",
        "              plot_one_box(xyxy, img0, label=label, color=colors[int(cls)], line_thickness=3)\n",
        "        \n",
        "        print(f\"{j+1}/{nframes} frames processed\")\n",
        "        output.write(img0)\n",
        "      else:\n",
        "        break\n",
        "    \n",
        "\n",
        "output.release()\n",
        "video.release()"
      ],
      "metadata": {
        "id": "kGoPWghGefaL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Where is the video?"
      ],
      "metadata": {
        "id": "MRygmq6qfVxE"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Oh... Here it is"
      ],
      "metadata": {
        "id": "WT8NLabsfY4L"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from IPython.display import HTML\n",
        "from base64 import b64encode\n",
        "import os\n",
        "\n",
        "save_path = '/content/gdrive/MyDrive/yolov7/output.mp4'\n",
        "\n",
        "compressed_path = \"/content/result_compressed.mp4\"\n",
        "\n",
        "os.system(f\"ffmpeg -i {save_path} -vcodec libx264 {compressed_path}\")\n",
        "\n",
        "mp4 = open(compressed_path,'rb').read()\n",
        "data_url = \"data:video/mp4;base64,\" + b64encode(mp4).decode()\n",
        "HTML(\"\"\"\n",
        "<video width=400 controls>\n",
        "      <source src=\"%s\" type=\"video/mp4\">\n",
        "</video>\n",
        "\"\"\" % data_url)"
      ],
      "metadata": {
        "id": "-5JRo4PVfbfa"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "You can also download"
      ],
      "metadata": {
        "id": "vDZ4SJKSfjFw"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import files\n",
        "save_path = '/content/gdrive/MyDrive/yolov7/output.mp4'\n",
        "files.download(save_path) "
      ],
      "metadata": {
        "id": "2hTZxtiRfke6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Single image processing**\n"
      ],
      "metadata": {
        "id": "1qQppKNqomPY"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%cd /content/gdrive/MyDrive/yolov7\n",
        "!gdown ##link here##\n"
      ],
      "metadata": {
        "id": "l04JTrXXov9U"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "source_image_path = '/content/gdrive/MyDrive/yolov7/dot.jpg' ## Make sure this is correct"
      ],
      "metadata": {
        "id": "4C-O-z-Qoz7o"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "with torch.no_grad():\n",
        "  weights, imgsz = opt['weights'], opt['img-size']\n",
        "  set_logging()\n",
        "  device = select_device(opt['device'])\n",
        "  half = device.type != 'cpu'\n",
        "  model = attempt_load(weights, map_location=device)\n",
        "  stride = int(model.stride.max())\n",
        "  imgsz = check_img_size(imgsz, s=stride) \n",
        "  if half:\n",
        "    model.half()\n",
        "\n",
        "  names = model.module.names if hasattr(model, 'module') else model.names\n",
        "  colors = [[random.randint(0, 255) for _ in range(3)] for _ in names]\n",
        "  if device.type != 'cpu':\n",
        "    model(torch.zeros(1, 3, imgsz, imgsz).to(device).type_as(next(model.parameters())))\n",
        "\n",
        "  img0 = cv2.imread(source_image_path)\n",
        "  img = letterbox(img0, imgsz, stride=stride)[0]\n",
        "  img = img[:, :, ::-1].transpose(2, 0, 1)  # BGR to RGB, to 3x416x416\n",
        "  img = np.ascontiguousarray(img)\n",
        "  img = torch.from_numpy(img).to(device)\n",
        "  img = img.half() if half else img.float()  # uint8 to fp16/32\n",
        "  img /= 255.0  # 0 - 255 to 0.0 - 1.0\n",
        "  if img.ndimension() == 3:\n",
        "    img = img.unsqueeze(0)\n",
        "\n",
        "  # Inference\n",
        "  t1 = time_synchronized()\n",
        "  pred = model(img, augment= False)[0]\n",
        "\n",
        "  # Apply NMS\n",
        "  classes = None\n",
        "  if opt['classes']:\n",
        "    classes = []\n",
        "    for class_name in opt['classes']:\n",
        "\n",
        "      classes.append(opt['classes'].index(class_name))\n",
        "\n",
        "\n",
        "  pred = non_max_suppression(pred, opt['conf-thres'], opt['iou-thres'], classes= classes, agnostic= False)\n",
        "  t2 = time_synchronized()\n",
        "  for i, det in enumerate(pred):\n",
        "    s = ''\n",
        "    s += '%gx%g ' % img.shape[2:]  # print string\n",
        "    gn = torch.tensor(img0.shape)[[1, 0, 1, 0]]\n",
        "    if len(det):\n",
        "      det[:, :4] = scale_coords(img.shape[2:], det[:, :4], img0.shape).round()\n",
        "\n",
        "      for c in det[:, -1].unique():\n",
        "        n = (det[:, -1] == c).sum()  # detections per class\n",
        "        s += f\"{n} {names[int(c)]}{'s' * (n > 1)}, \"  # add to string\n",
        "    \n",
        "      for *xyxy, conf, cls in reversed(det):\n",
        "\n",
        "        label = f'{names[int(cls)]} {conf:.2f}'\n",
        "        plot_one_box(xyxy, img0, label=label, color=colors[int(cls)], line_thickness=3)\n",
        "\n",
        "from google.colab.patches import cv2_imshow\n",
        "cv2_imshow(img0) ## display img"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 248
        },
        "id": "bUW38FRto7Zi",
        "outputId": "6376e628-6f1b-46a7-e9cd-84d7b1d5db9c"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-1-9747cae1ab49>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0;32mwith\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mno_grad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m   \u001b[0mweights\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mimgsz\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mopt\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'weights'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mopt\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'img-size'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m   \u001b[0mset_logging\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m   \u001b[0mdevice\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mselect_device\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mopt\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'device'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m   \u001b[0mhalf\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdevice\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtype\u001b[0m \u001b[0;34m!=\u001b[0m \u001b[0;34m'cpu'\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mNameError\u001b[0m: name 'torch' is not defined"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Live Video Processing**"
      ],
      "metadata": {
        "id": "IZt9RosNfuID"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#Just don't read this and assume it works because it does. There are still some things to flush out here. \n",
        "from IPython.display import display, Javascript, Image\n",
        "from google.colab.output import eval_js\n",
        "from google.colab.patches import cv2_imshow\n",
        "from base64 import b64decode, b64encode\n",
        "import PIL\n",
        "import io\n",
        "import html\n",
        "# function to convert the JavaScript object into an OpenCV image\n",
        "def js_to_image(js_reply):\n",
        "  \"\"\"\n",
        "  Params:\n",
        "          js_reply: JavaScript object containing image from webcam\n",
        "  Returns:\n",
        "          img: OpenCV BGR image\n",
        "  \"\"\"\n",
        "  # decode base64 image\n",
        "  image_bytes = b64decode(js_reply.split(',')[1])\n",
        "  # convert bytes to numpy array\n",
        "  jpg_as_np = np.frombuffer(image_bytes, dtype=np.uint8)\n",
        "  # decode numpy array into OpenCV BGR image\n",
        "  img = cv2.imdecode(jpg_as_np, flags=1)\n",
        "\n",
        "  return img\n",
        "\n",
        "# function to convert OpenCV Rectangle bounding box image into base64 byte string to be overlayed on video stream\n",
        "def bbox_to_bytes(bbox_array):\n",
        "  \"\"\"\n",
        "  Params:\n",
        "          bbox_array: Numpy array (pixels) containing rectangle to overlay on video stream.\n",
        "  Returns:\n",
        "        bytes: Base64 image byte string\n",
        "  \"\"\"\n",
        "  # convert array into PIL image\n",
        "  bbox_PIL = PIL.Image.fromarray(bbox_array, 'RGBA')\n",
        "  iobuf = io.BytesIO()\n",
        "  # format bbox into png for return\n",
        "  bbox_PIL.save(iobuf, format='png')\n",
        "  # format return string\n",
        "  bbox_bytes = 'data:image/png;base64,{}'.format((str(b64encode(iobuf.getvalue()), 'utf-8')))\n",
        "\n",
        "  return bbox_bytes\n",
        "\n",
        "\n",
        "# JavaScript to properly create our live video stream using our webcam as input\n",
        "def video_stream():\n",
        "  js = Javascript('''\n",
        "    var video;\n",
        "    var div = null;\n",
        "    var stream;\n",
        "    var captureCanvas;\n",
        "    var imgElement;\n",
        "    var labelElement;\n",
        "    \n",
        "    var pendingResolve = null;\n",
        "    var shutdown = false;\n",
        "    \n",
        "    function removeDom() {\n",
        "       stream.getVideoTracks()[0].stop();\n",
        "       video.remove();\n",
        "       div.remove();\n",
        "       video = null;\n",
        "       div = null;\n",
        "       stream = null;\n",
        "       imgElement = null;\n",
        "       captureCanvas = null;\n",
        "       labelElement = null;\n",
        "    }\n",
        "    \n",
        "    function onAnimationFrame() {\n",
        "      if (!shutdown) {\n",
        "        window.requestAnimationFrame(onAnimationFrame);\n",
        "      }\n",
        "      if (pendingResolve) {\n",
        "        var result = \"\";\n",
        "        if (!shutdown) {\n",
        "          captureCanvas.getContext('2d').drawImage(video, 0, 0, 640, 480);\n",
        "          result = captureCanvas.toDataURL('image/jpeg', 0.8)\n",
        "        }\n",
        "        var lp = pendingResolve;\n",
        "        pendingResolve = null;\n",
        "        lp(result);\n",
        "      }\n",
        "    }\n",
        "    \n",
        "    async function createDom() {\n",
        "      if (div !== null) {\n",
        "        return stream;\n",
        "      }\n",
        "\n",
        "      div = document.createElement('div');\n",
        "      div.style.border = '2px solid black';\n",
        "      div.style.padding = '3px';\n",
        "      div.style.width = '100%';\n",
        "      div.style.maxWidth = '600px';\n",
        "      document.body.appendChild(div);\n",
        "      \n",
        "      const modelOut = document.createElement('div');\n",
        "      modelOut.innerHTML = \"<span>Status:</span>\";\n",
        "      labelElement = document.createElement('span');\n",
        "      labelElement.innerText = 'No data';\n",
        "      labelElement.style.fontWeight = 'bold';\n",
        "      modelOut.appendChild(labelElement);\n",
        "      div.appendChild(modelOut);\n",
        "           \n",
        "      video = document.createElement('video');\n",
        "      video.style.display = 'block';\n",
        "      video.width = div.clientWidth - 6;\n",
        "      video.setAttribute('playsinline', '');\n",
        "      video.onclick = () => { shutdown = true; };\n",
        "      stream = await navigator.mediaDevices.getUserMedia(\n",
        "          {video: { facingMode: \"environment\"}});\n",
        "      div.appendChild(video);\n",
        "\n",
        "      imgElement = document.createElement('img');\n",
        "      imgElement.style.position = 'absolute';\n",
        "      imgElement.style.zIndex = 1;\n",
        "      imgElement.onclick = () => { shutdown = true; };\n",
        "      div.appendChild(imgElement);\n",
        "      \n",
        "      const instruction = document.createElement('div');\n",
        "      instruction.innerHTML = \n",
        "          '<span style=\"color: red; font-weight: bold;\">' +\n",
        "          'When finished, click here or on the video to stop this demo</span>';\n",
        "      div.appendChild(instruction);\n",
        "      instruction.onclick = () => { shutdown = true; };\n",
        "      \n",
        "      video.srcObject = stream;\n",
        "      await video.play();\n",
        "\n",
        "      captureCanvas = document.createElement('canvas');\n",
        "      captureCanvas.width = 640; //video.videoWidth;\n",
        "      captureCanvas.height = 480; //video.videoHeight;\n",
        "      window.requestAnimationFrame(onAnimationFrame);\n",
        "      \n",
        "      return stream;\n",
        "    }\n",
        "    async function stream_frame(label, imgData) {\n",
        "      if (shutdown) {\n",
        "        removeDom();\n",
        "        shutdown = false;\n",
        "        return '';\n",
        "      }\n",
        "\n",
        "      var preCreate = Date.now();\n",
        "      stream = await createDom();\n",
        "      \n",
        "      var preShow = Date.now();\n",
        "      if (label != \"\") {\n",
        "        labelElement.innerHTML = label;\n",
        "      }\n",
        "            \n",
        "      if (imgData != \"\") {\n",
        "        var videoRect = video.getClientRects()[0];\n",
        "        imgElement.style.top = videoRect.top + \"px\";\n",
        "        imgElement.style.left = videoRect.left + \"px\";\n",
        "        imgElement.style.width = videoRect.width + \"px\";\n",
        "        imgElement.style.height = videoRect.height + \"px\";\n",
        "        imgElement.src = imgData;\n",
        "      }\n",
        "      \n",
        "      var preCapture = Date.now();\n",
        "      var result = await new Promise(function(resolve, reject) {\n",
        "        pendingResolve = resolve;\n",
        "      });\n",
        "      shutdown = false;\n",
        "      \n",
        "      return {'create': preShow - preCreate, \n",
        "              'show': preCapture - preShow, \n",
        "              'capture': Date.now() - preCapture,\n",
        "              'img': result};\n",
        "    }\n",
        "    ''')\n",
        "\n",
        "  display(js)\n",
        "  \n",
        "def video_frame(label, bbox):\n",
        "  data = eval_js('stream_frame(\"{}\", \"{}\")'.format(label, bbox))\n",
        "  return data"
      ],
      "metadata": {
        "id": "rwQwLizXf0Us"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "video_stream()\n",
        "label_html = 'Loading...'\n",
        "bbox = ''\n",
        "count = 0 \n",
        "\n",
        "with torch.no_grad():\n",
        "  weights, imgsz = opt['weights'], (480,640)\n",
        "  set_logging()\n",
        "  device = select_device(opt['device'])\n",
        "  half = device.type != 'cpu'\n",
        "  model = attempt_load(weights, map_location=device)\n",
        "  stride = int(model.stride.max())\n",
        "\n",
        "  if half:\n",
        "    model.half()\n",
        "\n",
        "  names = model.module.names if hasattr(model, 'module') else model.names\n",
        "  colors = [[random.randint(0, 255) for _ in range(3)] for _ in names]\n",
        "  if device.type != 'cpu':\n",
        "    model(torch.zeros(1, 3, imgsz[0], imgsz[1]).to(device).type_as(next(model.parameters())))\n",
        "  classes = None\n",
        "  if opt['classes']:\n",
        "    classes = []\n",
        "    for class_name in opt['classes']:\n",
        "      classes.append(opt['classes'].index(class_name))\n",
        "  \n",
        "  while True:\n",
        "    js_reply = video_frame(label_html, bbox)\n",
        "    if not js_reply:\n",
        "        break\n",
        "    \n",
        "    img0 = js_to_image(js_reply[\"img\"])\n",
        "    bbox_array = np.zeros([480,640,4], dtype=np.uint8)\n",
        "    img = letterbox(img0, imgsz, stride=stride)[0]\n",
        "    img = img[:, :, ::-1].transpose(2, 0, 1)  # BGR to RGB, to 3x416x416\n",
        "    img = np.ascontiguousarray(img)\n",
        "    img = torch.from_numpy(img).to(device)\n",
        "    img = img.half() if half else img.float()  # uint8 to fp16/32\n",
        "    img /= 255.0  # 0 - 255 to 0.0 - 1.0\n",
        "    if img.ndimension() == 3:\n",
        "      img = img.unsqueeze(0)\n",
        "\n",
        "    # Inference\n",
        "    t1 = time_synchronized()\n",
        "    pred = model(img, augment= False)[0]\n",
        "\n",
        "    # Apply NMS\n",
        "    pred = non_max_suppression(pred, opt['conf-thres'], opt['iou-thres'], classes= classes, agnostic= False)\n",
        "    t2 = time_synchronized()\n",
        "    for i, det in enumerate(pred):\n",
        "      s = ''\n",
        "      s += '%gx%g ' % img.shape[2:]  # print string\n",
        "      gn = torch.tensor(img0.shape)[[1, 0, 1, 0]]\n",
        "      if len(det):\n",
        "        det[:, :4] = scale_coords(img.shape[2:], det[:, :4], img0.shape).round()\n",
        "\n",
        "        for c in det[:, -1].unique():\n",
        "          n = (det[:, -1] == c).sum()  # detections per class\n",
        "          s += f\"{n} {names[int(c)]}{'s' * (n > 1)}, \"  # add to string\n",
        "    \n",
        "        for *xyxy, conf, cls in reversed(det):\n",
        "\n",
        "          label = f'{names[int(cls)]} {conf:.2f}'\n",
        "          plot_one_box(xyxy, bbox_array, label=label, color=colors[int(cls)], line_thickness=3)\n",
        "    \n",
        "    bbox_array[:,:,3] = (bbox_array.max(axis = 2) > 0 ).astype(int) * 255\n",
        "    bbox_bytes = bbox_to_bytes(bbox_array)\n",
        "    \n",
        "    bbox = bbox_bytes\n"
      ],
      "metadata": {
        "id": "6f2XrFTrgEuC"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}